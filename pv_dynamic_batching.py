# -*- coding: utf-8 -*-

"""
The script works by starting with the first PV and then searching if any other PV is lying around in the distance set by us, in this case 275. After finding this New PV it searches any other PV lying around under distance of 275 bps again and so on, until it cannot find any PV in the 275bps distance of the last PV added in the bin.

Bins are of variable length because of that. This means that - every PV in the bin is near to another PV by the distance of 275bps or less but the clusters/ bins are separated by a distance of 275 bps  or more.

As if any cluster/bin is in the distance of 275 bp or less of the another cluster/bin, Both will be merged according to the above criteria.

Input :  Data-set of Phased Variants overlapped with Cytoband and (Yes/No) AID Column
Output:  Dynamic Bins of Phased Varinats (input), 2 new columns are added.
"""


"""pv_dynamic_batching.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CpjIyWTDribZkFP6giIDtTh9LusYUrrW

Importing Libraries
"""

import pandas as pd
import math
import warnings
warnings.filterwarnings('ignore')

"""Loading CSV to a Dataframe object"""

df_ = pd.read_csv('maf2_aid_overlap2.csv')
df_.head()

df_['covers_aid_region'].value_counts()

"""Creating a dummy dataframe, so that we are not working on the original dataframe"""

dummy_df = df_.copy()
dummy_df.sort_values('start',inplace=True,ignore_index=True)
dummy_df.head(500)[300:]

"""Defining outer funtion, which filters the dataframe based on different chromosomes"""

def foo(df):
  Chromo_col = 'chr2'
  dum = df
  chrList = dum[Chromo_col].value_counts().index.tolist()
  for i in chrList:
    q = dum[dum[Chromo_col]==i]
    q.sort_values('Start_Position',inplace = True, ignore_index=True)
    batching(q)
    #print(q.shape[0],dum.shape[0])
    #break

"""Inner funtion to create multiple small bins based on distance between two potential phased variants.

*   For the first iteration, a small batch is created based on batchsize provided.
*   Then, function looks for adajacent batches within the batchsize provided.
*   Else, next small batch is created.
*   And so on.
"""

res = []
def batching(df):
  dum = df
  n = dum.shape[0]
  next_start_pos = 0
  BatchSize =275

  while n != 0:

    start_val = dum["Start_Position"][0] #- dum["Start_Position"][0]%170

    if len(res) > 0 and start_val <= next_start_pos + BatchSize :
      end_val = start_val + BatchSize
      filtererd_df = dum[dum["Start_Position"] <= end_val]

      if filtererd_df.shape[0] == 0:  # doing this, if the filtererd_df is empty (edge cases)
        next_start_pos = 0
        continue

      lS_ = filtererd_df.index[-1]
      last_batch = res.pop()

      Region_S = last_batch['Region Start'][0]
      filtererd_df['Region Start'] = Region_S
      merged_df = pd.concat([last_batch,filtererd_df],ignore_index=True)
      merged_df['Region End'] = (100 - filtererd_df['Start_Position'][lS_]%100) + filtererd_df['Start_Position'][lS_]
      res.append(merged_df)
      next_start_pos = filtererd_df['Start_Position'][lS_]

      dum = dum.iloc[lS_+1:]
      dum.reset_index(drop=True,inplace=True)
      n -= lS_+1

    else:
      end_val = start_val + BatchSize

      filtererd_df = dum[dum["Start_Position"] <= end_val]

      lS_ = filtererd_df.index[-1]
      filtererd_df['Region Start'] = filtererd_df['Start_Position'][0]-filtererd_df['Start_Position'][0]%100 #rounding down and storing region start val
      filtererd_df['Region End'] =   (100 - filtererd_df['Start_Position'][lS_]%100) + filtererd_df['Start_Position'][lS_] # rounding up and storing region end val
      res.append(filtererd_df)
      next_start_pos = filtererd_df['Start_Position'][lS_]

      dum = dum.iloc[lS_+1:]
      dum.reset_index(drop=True,inplace=True)
      n -= lS_+1
      #break

foo(dummy_df)

res[0]

len(res)

Binned_df = pd.concat(res)
Binned_df.reset_index(drop=True,inplace=True)

Binned_df.head()

Binned_df.to_csv("FinalDf_18Aug.csv",index=False)
